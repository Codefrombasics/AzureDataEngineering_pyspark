{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055b2d0e-ac6d-49a8-a4b3-c5238ec4b8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/04/11 10:22:11 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "25/04/11 10:22:12 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, explode, expr\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define schema\n",
    "myschema = StructType([\n",
    "    StructField(\"id\", LongType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", LongType(), True),\n",
    "    StructField(\"contacts\", StructType([\n",
    "        StructField(\"email\", StringType(), True),\n",
    "        StructField(\"phones\", ArrayType(StringType()), True)\n",
    "    ]), True),\n",
    "    StructField(\"skills\", ArrayType(StringType()), True),\n",
    "    StructField(\"projects\", ArrayType(StructType([\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"duration\", StringType(), True)\n",
    "    ])), True)\n",
    "])\n",
    "\n",
    "# Create Spark session\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Kafka to CSV\")\n",
    "    .master(\"local[*]\")\n",
    "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0')\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Read from Kafka\n",
    "kafka_df = (\n",
    "    spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "    .option(\"subscribe\", \"people\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Cast value to string and parse JSON\n",
    "json_df = kafka_df.selectExpr(\"cast(value as string) as json_string\")\n",
    "parsed_df = json_df.withColumn(\"data\", from_json(col(\"json_string\"), myschema)).select(\"data.*\")\n",
    "\n",
    "# Flatten nested structure\n",
    "df_flat = parsed_df \\\n",
    "    .withColumn(\"email\", col(\"contacts.email\")) \\\n",
    "    .withColumn(\"phones\", col(\"contacts.phones\")) \\\n",
    "    .withColumn(\"phone\", explode(\"phones\")) \\\n",
    "    .withColumn(\"skill\", explode(\"skills\")) \\\n",
    "    .withColumn(\"project\", explode(\"projects\")) \\\n",
    "    .withColumn(\"project_name\", col(\"project.name\")) \\\n",
    "    .withColumn(\"project_duration\", col(\"project.duration\")) \\\n",
    "    .drop(\"contacts\", \"phones\", \"skills\", \"projects\", \"project\")\n",
    "\n",
    "# Write to CSV\n",
    "(\n",
    "    df_flat.writeStream\n",
    "    .format(\"csv\")\n",
    "    .option(\"path\", \"data/output/\")\n",
    "    .option(\"checkpointLocation\", \"checkpoint_dir_csv\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    "    .awaitTermination()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e934add7-f69f-46e9-8dd6-5915842a44ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the kafka_df to read from kafka\n",
    "\n",
    "kafka_df = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "    .option(\"subscribe\", \"people\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3043cb9c-b820-4c8e-a063-272669cfb5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c89d3f-dce0-4f47-a686-d1f38eebc27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, explode, col,from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, ArrayType\n",
    "\n",
    "# Cast Kafka value to string\n",
    "kafka_json_df = kafka_df.withColumn(\"value\", expr(\"CAST(value AS STRING)\"))\n",
    "\n",
    "# Define schema manually or use existing\n",
    "myschema = StructType([\n",
    "    StructField(\"id\", LongType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", LongType(), True),\n",
    "    StructField(\"contacts\", StructType([\n",
    "        StructField(\"email\", StringType(), True),\n",
    "        StructField(\"phones\", ArrayType(StringType()), True)\n",
    "    ]), True),\n",
    "    StructField(\"skills\", ArrayType(StringType()), True),\n",
    "    StructField(\"projects\", ArrayType(StructType([\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"duration\", StringType(), True)\n",
    "    ])), True)\n",
    "])\n",
    "\n",
    "# Parse JSON\n",
    "streaming_df = kafka_json_df.withColumn(\"data\", from_json(col(\"value\"), myschema)).select(\"data.*\")\n",
    "\n",
    "# Explode and group\n",
    "# skill_counts = streaming_df.withColumn(\"skill\", explode(\"skills\")) \\\n",
    "#                            .groupBy(\"skill\") \\\n",
    "#                            .count()\n",
    "\n",
    "# Write in complete mode\n",
    "query = (\n",
    "    skill_counts\n",
    "    .writeStream\n",
    "    .format(\"csv\")\n",
    "    .option(\"path\", \"data/output/\")\n",
    "    .option(\"checkpointLocation\", \"checkpoint_dir_kafka\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    "    .awaitTermination()\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec06ce56-6843-4fd0-ba91-c1a94c5e3b28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
