DataFrame
==========

RDD is a low level api for spark

DataFrame is a collection of Row objects, each Row objects represents a record

Df is like a SQL table. 
DF provide operations to SQL queries which is not present in RDD


Creating DataFrame:

We can create dataframe in 4 ways

1. From RDD
2. from local python objects
3. from the results of queries
4. from external datasources



1. Creating DataFrame from RDD---> RDD.toDF

spark.createDataFrame(RDD/list,schema)

we have to create sparkSession(spark session object)=> df=spark.createDataFrame(RDD)

sqlContext=> df=sqlContext.createDataFrame(RDD)




>>> x=[("Raja",2000),("James",3500)]
>>> spark
<pyspark.sql.session.SparkSession object at 0x7001523fbd70>
>>> df=spark.createDataFrame(x)
>>> df.show()
+-----+----+
|   _1|  _2|
+-----+----+
| Raja|2000|
|James|3500|
+-----+----+

>>> #create Datafram using Schema
>>> df=spark.createDataFrame(x,['Name','Salary'])
>>> df.show()

+-----+------+
| Name|Salary|
+-----+------+
| Raja|  2000|
|James|  3500|
+-----+------+

>>> df.count()
2
>>> df.printSchema()
root
 |-- Name: string (nullable = true)
 |-- Salary: long (nullable = true)


>>> #using Dictionary, which provided Schema
>>> mydict=[{'name':'Ajay','age':25},{'name':'Vijay','age':30}]


>>> df=spark.createDataFrame(mydict)
>>> df.show()
+---+-----+
|age| name|
+---+-----+
| 25| Ajay|
| 30|Vijay|
+---+-----+

>>> mydict=[{'name':'Ajay','age':25},{'name':'Vijay','age':30,'city':'Chennai'}]>>> df=spark.createDataFrame(mydict)
>>> df.show()
+---+-----+-------+
|age| name|   city|
+---+-----+-------+
| 25| Ajay|   null|
| 30|Vijay|Chennai|
+---+-----+-------+

>>> #creating a Dataframe from RDD
>>> x=[("Raja",2000),("James",3500)]
>>> rdd=sc.parallelize(x)
>>> df1=spark.createDataFrame(rdd)
>>> df1.show()
+-----+----+
|   _1|  _2|
+-----+----+
| Raja|2000|
|James|3500|
+-----+----+

>>> df1=spark.createDataFrame(rdd,['name','age'])
>>> df1.show()
+-----+----+
| name| age|
+-----+----+
| Raja|2000|
|James|3500|
+-----+----+

>>> spark
<pyspark.sql.session.SparkSession object at 0x7001523fbd70>
>>> sc
<SparkContext master=local[*] appName=PySparkShell>
>>> 
>>> #creating DataFrame using Row Object
>>> x=[("Raja",2000),("James",3500)]
>>> rdd=sc.parallelize(x)
>>> from pyspark.sql import Row
>>> employee=Row('Name','Age')
>>> type(employee)
<class 'pyspark.sql.types.Row'>
>>> employee=Row('Name','Salary')
>>> 
>>> type(employee)
<class 'pyspark.sql.types.Row'>
>>> emp=rdd.map(lambda p:employee(*p))
>>> emp.collect()
[Row(Name='Raja', Salary=2000), Row(Name='James', Salary=3500)]
>>> df=spark.createDataFrame(emp)
>>> df.show()
+-----+------+
| Name|Salary|
+-----+------+
| Raja|  2000|
|James|  3500|
+-----+------+

>>> df.printSchema()
root
 |-- Name: string (nullable = true)
 |-- Salary: long (nullable = true)

>>> #providing shcem using StructType
>>> from pyspark.sql.types import *
>>> myschema=StructType([StructField("Name",StringType(),True),StructField("Salary",LongType(),True)])
>>> df=spark.createDataFrame(rdd,myschema)
>>> df.show()
+-----+------+
| Name|Salary|
+-----+------+
| Raja|  2000|
|James|  3500|
+-----+------+

>>> df.printSchema()
root
 |-- Name: string (nullable = true)
 |-- Salary: long (nullable = true)

>>> #passing schema as string
>>> df=spark.createDataFrame(rdd,"Empname:string,EmpSalary:long")
>>> df.show()
+-------+---------+
|Empname|EmpSalary|
+-------+---------+
|   Raja|     2000|
|  James|     3500|
+-------+---------+

>>> df.printSchema()
root
 |-- Empname: string (nullable = true)
 |-- EmpSalary: long (nullable = true)


>>> df.rdd.getNumPartitions()
2


creating emp records

empdata=[(101,'Mohan',10000,'m',11),
         (102,'Ragu',20000,'m',12),
		 (103,'Hari',15000,'m',12),
		 (104,'Harini',45000,'f',13),
		 (105,'Nancy',23000,'f',11)]

>>> empdata=[(101,'Mohan',10000,'m',11),
...          (102,'Ragu',20000,'m',12),
...          (103,'Hari',15000,'m',12),
...         (104,'Harini',45000,'f',13),
...      (105,'Nancy',23000,'f',11)]
>>> rdd=sc.parallelize(empdata)
>>> df=spark.createDataFrame(rdd,['eid','ename','sal','gender','deptno'])
>>> df.show()
+---+------+-----+------+------+
|eid| ename|  sal|gender|deptno|
+---+------+-----+------+------+
|101| Mohan|10000|     m|    11|
|102|  Ragu|20000|     m|    12|
|103|  Hari|15000|     m|    12|
|104|Harini|45000|     f|    13|
|105| Nancy|23000|     f|    11|
+---+------+-----+------+------+

>>> df.collect()
[Row(eid=101, ename='Mohan', sal=10000, gender='m', deptno=11), Row(eid=102, ename='Ragu', sal=20000, gender='m', deptno=12), Row(eid=103, ename='Hari', sal=15000, gender='m', deptno=12), Row(eid=104, ename='Harini', sal=45000, gender='f', deptno=13), Row(eid=105, ename='Nancy', sal=23000, gender='f', deptno=11)]


>>> rdd1=sc.textFile("hdfs://localhost:9000/dineshnew/emp2.txt")
>>> rdd1.collect()
['101,Hariharan,10000,m,11', '102,Kalai,20000,f,12', '103,Sivam,30000,m,11', '104,Blake,35000,m,12', '105,James,40000,m,11', '106,Miller,45000,m,13', '107,Alice,50000,f,13']
>>> rdd2=rdd1.map(lambda x:x.split(","))
>>> 
>>> rdd2.collect()
[['101', 'Hariharan', '10000', 'm', '11'], ['102', 'Kalai', '20000', 'f', '12'], ['103', 'Sivam', '30000', 'm', '11'], ['104', 'Blake', '35000', 'm', '12'], ['105', 'James', '40000', 'm', '11'], ['106', 'Miller', '45000', 'm', '13'], ['107', 'Alice', '50000', 'f', '13']]

>>> r3=rdd2.map(lambda x:Row(eid=int(x[0]),ename=x[1],sal=int(x[2]),gen=x[3],dno=int(x[4])))
>>> r3.collect()
[Row(eid=101, ename='Hariharan', sal=10000, gen='m', dno=11), Row(eid=102, ename='Kalai', sal=20000, gen='f', dno=12), Row(eid=103, ename='Sivam', sal=30000, gen='m', dno=11), Row(eid=104, ename='Blake', sal=35000, gen='m', dno=12), Row(eid=105, ename='James', sal=40000, gen='m', dno=11), Row(eid=106, ename='Miller', sal=45000, gen='m', dno=13), Row(eid=107, ename='Alice', sal=50000, gen='f', dno=13)]
>>> df=spark.createDataFrame(r3)
>>> df.show()
+---+---------+-----+---+---+
|eid|    ename|  sal|gen|dno|
+---+---------+-----+---+---+
|101|Hariharan|10000|  m| 11|
|102|    Kalai|20000|  f| 12|
|103|    Sivam|30000|  m| 11|
|104|    Blake|35000|  m| 12|
|105|    James|40000|  m| 11|
|106|   Miller|45000|  m| 13|
|107|    Alice|50000|  f| 13|
+---+---------+-----+---+---+

>>> df.eid
Column<'eid'>
>>> df.ename
Column<'ename'>
>>> df.select("eid").show()
+---+
|eid|
+---+
|101|
|102|
|103|
|104|
|105|
|106|
|107|
+---+



